{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "348b9fab-0f72-2597-e602-02370ae1b03e"
   },
   "source": [
    "# Shapelets for time series classification\n",
    "\n",
    "Hello everyone, in this notebook, I would like to show an implementation of [shapelet feature extraction](http://alumni.cs.ucr.edu/~lexiangy/Shapelet/kdd2009shapelet.pdf). The nice thing about this extraction technique is that it is somewhat interpretable and delivers some insight. Moreover, this technique can be applied on all 1-D series (time-series or converted contours of objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1445852a-87ce-204d-5d9a-7f4b68758376",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.image as mpimg       # reading images to numpy arrays\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt        # to plot any graph\n",
    "\n",
    "from skimage import measure            # to find shape contour\n",
    "import scipy.ndimage as ndi            # to determine shape centrality\n",
    "\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = (6, 6)      # setting default size of plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5f4d2772-f45e-724b-194b-ec2a80a3903f"
   },
   "source": [
    "## Step 1: Converting an object contour to a 1D signal\n",
    "Credits to who they belong: thank you [Lorinc](https://www.kaggle.com/lorinc/leaf-classification/feature-extraction-from-images) for your amazing notebook! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a82413be-0e4a-407d-4506-0c8196947854",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_leaf(image):\n",
    "    img = mpimg.imread(image)\n",
    "    cy, cx = ndi.center_of_mass(img)\n",
    "    return img, (cx, cy)\n",
    "\n",
    "\n",
    "def get_contour(img, thresh=.8):\n",
    "    contours = measure.find_contours(img, thresh)\n",
    "    return max(contours, key=len)  # Take longest one\n",
    "\n",
    "\n",
    "def convert_to_1d(file, sample=250, thresh=.8, plot=False, norm=True):\n",
    "    img, (cx, cy) = draw_leaf(file)\n",
    "    contour = get_contour(img, thresh)\n",
    "    distances = [manhattan_distance([cx, cy], [contour[i][0], contour[i][1]]) for i in range(0,len(contour),sample)]\n",
    "    distances.extend(distances)\n",
    "    if plot:\n",
    "        f, axarr = plt.subplots(2, sharex=False) # , sharex=True\n",
    "        axarr[0].imshow(img, cmap='Set3')\n",
    "        axarr[0].plot(contour[::, 1], contour[::, 0], linewidth=0.5)\n",
    "        axarr[0].scatter(cx, cy)\n",
    "        axarr[1].plot(distances)\n",
    "        plt.show()\n",
    "    if norm:\n",
    "        return np.divide(distances, max(distances))\n",
    "    else:\n",
    "        return distances  #  Extend it twice so that it is cyclic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3816a597-aff7-9a46-c196-3afe1211fbf5"
   },
   "source": [
    "## Step 2: Shapelet extraction code\n",
    "I'm planning to make some python library from this soon, as I could not find one.. The current drawback is that the complexity is still too high to be applicable in a big data setting. All possible optimization are more than welcome!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d73f6560-3e4c-4828-4f92-5b484eb4c679",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_candidates(data, max_len=5, min_len=2):\n",
    "    candidates, l = [], max_len\n",
    "    while l >= min_len:\n",
    "        for i in range(len(data)):\n",
    "            time_serie, label = data[i][0], data[i][1]\n",
    "            for k in range(len(time_serie)-l+1): candidates.append((time_serie[k:k+l], label))\n",
    "        l -= 1\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def check_candidate(data, shapelet):\n",
    "    histogram = {} \n",
    "    for entry in data:\n",
    "        # TODO: entropy pre-pruning in each iteration\n",
    "        time_serie, label = entry[0], entry[1]\n",
    "        d, idx = subsequence_dist(time_serie, shapelet)\n",
    "        if d is not None:\n",
    "            histogram[d] = [(time_serie, label)] if d not in histogram else histogram[d].append((time_serie, label))\n",
    "    return find_best_split_point(histogram)\n",
    "\n",
    "\n",
    "def calculate_dict_entropy(data):\n",
    "    counts = {}\n",
    "    for entry in data:\n",
    "        if entry[1] in counts: counts[entry[1]] += 1\n",
    "        else: counts[entry[1]] = 1\n",
    "    return calculate_entropy(np.divide(list(counts.values()), float(sum(list(counts.values())))))\n",
    "\n",
    "\n",
    "def find_best_split_point(histogram):\n",
    "    histogram_values = list(itertools.chain.from_iterable(list(histogram.values())))\n",
    "    prior_entropy = calculate_dict_entropy(histogram_values)\n",
    "    best_distance, max_ig = 0, 0\n",
    "    best_left, best_right = None, None\n",
    "    for distance in histogram:\n",
    "        data_left = []\n",
    "        data_right = []\n",
    "        for distance2 in histogram:\n",
    "            if distance2 <= distance: data_left.extend(histogram[distance2])\n",
    "            else: data_right.extend(histogram[distance2])\n",
    "        ig = prior_entropy - (float(len(data_left))/float(len(histogram_values))*calculate_dict_entropy(data_left) + \\\n",
    "             float(len(data_right))/float(len(histogram_values)) * calculate_dict_entropy(data_right))\n",
    "        if ig > max_ig: best_distance, max_ig, best_left, best_right = distance, ig, data_left, data_right\n",
    "    return max_ig, best_distance, best_left, best_right\n",
    "\n",
    "\n",
    "def manhattan_distance(a, b, min_dist=float('inf')):\n",
    "    dist = 0\n",
    "    for x, y in zip(a, b):\n",
    "        dist += np.abs(float(x)-float(y))\n",
    "        if dist >= min_dist: return None\n",
    "    return dist\n",
    "\n",
    "def calculate_entropy(probabilities):\n",
    "    return sum([-prob * np.log(prob)/np.log(2) if prob != 0 else 0 for prob in probabilities])\n",
    "\n",
    "\n",
    "def subsequence_dist(time_serie, sub_serie):\n",
    "    if len(sub_serie) < len(time_serie):\n",
    "        min_dist, min_idx = float(\"inf\"), 0\n",
    "        for i in range(len(time_serie)-len(sub_serie)+1):\n",
    "            dist = manhattan_distance(sub_serie, time_serie[i:i+len(sub_serie)], min_dist)\n",
    "            if dist is not None and dist < min_dist: min_dist, min_idx = dist, i\n",
    "        return min_dist, min_idx\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def find_shapelets_bf(data, max_len=100, min_len=1, plot=True, verbose=True):\n",
    "    candidates = generate_candidates(data, max_len, min_len)\n",
    "    bsf_gain, bsf_shapelet = 0, None\n",
    "    if verbose: candidates_length = len(candidates)\n",
    "    for idx, candidate in enumerate(candidates):\n",
    "        gain, dist, data_left, data_right = check_candidate(data, candidate[0])\n",
    "        if verbose: print(idx, '/', candidates_length, \":\", gain, dist)\n",
    "        if gain > bsf_gain:\n",
    "            bsf_gain, bsf_shapelet = gain, candidate[0]\n",
    "            if verbose:\n",
    "                print('Found new best shapelet with gain & dist:', bsf_gain, dist, [x[1] for x in data_left], \\\n",
    "                                                                                   [x[1] for x in data_right])\n",
    "            if plot:\n",
    "                plt.plot(bsf_shapelet)\n",
    "                plt.show()\n",
    "            plt.show()\n",
    "    return bsf_shapelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1367c4c0-0a97-f2cd-c5df-d6a34d5b993b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_shapelets(data, min_len=10, max_len=15, verbose=1):\n",
    "    _classes = np.unique([x[1] for x in data])\n",
    "    shapelet_dict = {}\n",
    "    for _class in _classes:\n",
    "        print('Extracting shapelets for', _class)\n",
    "        transformed_data = []\n",
    "        for entry in data:\n",
    "            print('Entry: ', entry)\n",
    "            time_serie, label = entry[0], entry[1]\n",
    "            if label == _class: transformed_data.append((time_serie, 1))\n",
    "            else: transformed_data.append((time_serie, 0))\n",
    "        shapelet_dict[_class] = find_shapelets_bf(transformed_data, max_len=max_len, min_len=min_len, plot=0, verbose=1)\n",
    "    return shapelet_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1ae7711e-fb9a-d2ae-308e-3f0d95aaf758"
   },
   "source": [
    "## Step 3: loading a subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5d305d28-d413-bd88-4008-0f80b10a370d",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "leaf_img = [('Acer Palmatum', [27, 118, 203, 324, 960, 1041]), \n",
    "            ('Acer Pictum', [146, 311, 362, 810, 915, 949, 956]),\n",
    "            ('Quercus Coccinea', [163, 189, 469, 510, 576, 605]),\n",
    "            ('Quercus Rhysophylla', [375, 481, 876, 1120, 1163, 1323]),\n",
    "            ('Salix Fragilis', [15, 620, 704, 847, 976, 1025])]\n",
    "leaf_map = {'Acer Palmatum': 0, 'Acer Pictum': 1, 'Salix Fragilis': 2,\n",
    "            'Quercus Rhysophylla': 3, 'Quercus Coccinea': 4}\n",
    "data = []\n",
    "\n",
    "for img in leaf_img:\n",
    "    name, image_numbers = img[0], img[1]\n",
    "    for number in image_numbers:\n",
    "        data.append((convert_to_1d('./input/images/'+str(number)+'.jpg', plot=0), \n",
    "                     leaf_map[name]))\n",
    "        \n",
    "convert_to_1d('./input/images/27.jpg', plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[5:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shapelet_dict = extract_shapelets(data[5:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shapelet_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2872fabb-9cb2-b705-7322-79eb7c0c160d"
   },
   "source": [
    "## Step 4: creating some plots\n",
    "For each of the five different leaves, we transform our dataset to a binary classification problem (one-vs-all) and find the most discriminative/characterizing shapelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distances1 = convert_to_1d('./input/images/27.jpg', plot=0, norm=1)\n",
    "distances1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(5, sharex=True) # , sharex=True\n",
    "axarr[0].plot(list(range(len(distances1))), distances1)\n",
    "_dist, _idx = subsequence_dist(distances1, shapelet_dict[0])\n",
    "axarr[0].plot(list(range(_idx, _idx+len(shapelet_dict[0]))), shapelet_dict[0], color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "26c3b4d3-58b3-a1c1-200a-5469c7bd2660",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shapelet_dict = extract_shapelets(data)\n",
    "\n",
    "distances1 = convert_to_1d('./input/images/27.jpg', plot=0, norm=1)\n",
    "distances2 = convert_to_1d('./input/images/146.jpg', plot=0, norm=1)\n",
    "distances3 = convert_to_1d('./input/images/375.jpg', plot=0, norm=1)\n",
    "distances4 = convert_to_1d('./input/images/15.jpg', plot=0, norm=1)\n",
    "distances5 = convert_to_1d('./input/images/163.jpg', plot=0, norm=1)\n",
    "\n",
    "f, axarr = plt.subplots(5, sharex=True) # , sharex=True\n",
    "\n",
    "axarr[0].plot(list(range(len(distances1))), distances1)\n",
    "_dist, _idx = subsequence_dist(distances1, shapelet_dict[0])\n",
    "axarr[0].plot(list(range(_idx, _idx+len(shapelet_dict[0]))), shapelet_dict[0], color='r')\n",
    "\n",
    "axarr[1].plot(distances2)\n",
    "_dist, _idx = subsequence_dist(distances2, shapelet_dict[1])\n",
    "axarr[1].plot(list(range(_idx, _idx+len(shapelet_dict[1]))), shapelet_dict[1], color='r')\n",
    "\n",
    "axarr[2].plot(distances3)\n",
    "_dist, _idx = subsequence_dist(distances3, shapelet_dict[2])\n",
    "axarr[2].plot(list(range(_idx, _idx+len(shapelet_dict[2]))), shapelet_dict[2], color='r')\n",
    "\n",
    "axarr[3].plot(distances4)\n",
    "_dist, _idx = subsequence_dist(distances4, shapelet_dict[3])\n",
    "axarr[3].plot(list(range(_idx, _idx+len(shapelet_dict[3]))), shapelet_dict[3], color='r')\n",
    "\n",
    "axarr[4].plot(distances5)\n",
    "_dist, _idx = subsequence_dist(distances5, shapelet_dict[4])\n",
    "axarr[4].plot(list(range(_idx, _idx+len(shapelet_dict[4]))), shapelet_dict[4], color='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6e3af401-ec8a-bc40-73ce-1bc029055d61"
   },
   "source": [
    "Except for the 4th time-serie, the shapelets seem to be quite characteristic for the leafs. Now we can use the minimal distance to each of these shapelets as features. Hope you enjoyed it.\n",
    "\n",
    "**Feedback is always welcome!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "939574d2-75dd-32e7-55ce-4233a4a11950",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 1613,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
