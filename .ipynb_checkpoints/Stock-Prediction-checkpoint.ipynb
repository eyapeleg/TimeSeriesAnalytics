{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Author: Gael Varoquaux gael.varoquaux@normalesup.org\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "     from matplotlib.finance import quotes_historical_yahoo_ochl\n",
    "except ImportError:\n",
    "     # quotes_historical_yahoo_ochl was named quotes_historical_yahoo before matplotlib 1.4\n",
    "    from matplotlib.finance import quotes_historical_yahoo as quotes_historical_yahoo_ochl\n",
    "from matplotlib.collections import LineCollection\n",
    "from sklearn import cluster, covariance, manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose a time period reasonably calm (not too long ago so that we get\n",
    "# high-tech firms, and before the 2008 crash)\n",
    "d1 = datetime.datetime(2014, 12, 1)\n",
    "d2 = datetime.datetime(2015, 1, 1)\n",
    "\n",
    "# kraft symbol has now changed from KFT to MDLZ in yahoo\n",
    "symbol_dict = {\n",
    "    'TOT': 'Total',\n",
    "    'XOM': 'Exxon',\n",
    "    'CVX': 'Chevron',\n",
    "    'COP': 'ConocoPhillips',\n",
    "    'VLO': 'Valero Energy',\n",
    "    'MSFT': 'Microsoft',\n",
    "    'IBM': 'IBM',\n",
    "    'TWX': 'Time Warner',\n",
    "    'CMCSA': 'Comcast',\n",
    "    'CVC': 'Cablevision',\n",
    "    'YHOO': 'Yahoo',\n",
    "    'HPQ': 'HP',\n",
    "    'AMZN': 'Amazon',\n",
    "    'TM': 'Toyota',\n",
    "    'CAJ': 'Canon',\n",
    "    'MTU': 'Mitsubishi',\n",
    "    'SNE': 'Sony',\n",
    "    'F': 'Ford',\n",
    "    'HMC': 'Honda',\n",
    "    'NAV': 'Navistar',\n",
    "    'NOC': 'Northrop Grumman',\n",
    "    'BA': 'Boeing',\n",
    "    'KO': 'Coca Cola',\n",
    "    'MMM': '3M',\n",
    "    'MCD': 'Mc Donalds',\n",
    "    'PEP': 'Pepsi',\n",
    "    'MDLZ': 'Kraft Foods',\n",
    "    'K': 'Kellogg',\n",
    "    'UN': 'Unilever',\n",
    "    'MAR': 'Marriott',\n",
    "    'PG': 'Procter Gamble',\n",
    "    'CL': 'Colgate-Palmolive',\n",
    "    'GE': 'General Electrics',\n",
    "    'WFC': 'Wells Fargo',\n",
    "    'JPM': 'JPMorgan Chase',\n",
    "    'AIG': 'AIG',\n",
    "    'AXP': 'American express',\n",
    "    'BAC': 'Bank of America',\n",
    "    'GS': 'Goldman Sachs',\n",
    "    'AAPL': 'Apple',\n",
    "    'SAP': 'SAP',\n",
    "    'CSCO': 'Cisco',\n",
    "    'TXN': 'Texas instruments',\n",
    "    'XRX': 'Xerox',\n",
    "    'LMT': 'Lookheed Martin',\n",
    "    'WMT': 'Wal-Mart',\n",
    "    'WBA': 'Walgreen',\n",
    "    'HD': 'Home Depot',\n",
    "    'GSK': 'GlaxoSmithKline',\n",
    "    'PFE': 'Pfizer',\n",
    "    'SNY': 'Sanofi-Aventis',\n",
    "    'NVS': 'Novartis',\n",
    "    'KMB': 'Kimberly-Clark',\n",
    "    'R': 'Ryder',\n",
    "    'GD': 'General Dynamics',\n",
    "    'RTN': 'Raytheon',\n",
    "    'CVS': 'CVS',\n",
    "    'CAT': 'Caterpillar',\n",
    "    'DD': 'DuPont de Nemours'}\n",
    "\n",
    "symbols, names = np.array(list(symbol_dict.items())).T\n",
    "\n",
    "quotes = [quotes_historical_yahoo_ochl(symbol, d1, d2, asobject=True)\n",
    "          for symbol in symbols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math as math\n",
    "\n",
    "def DTWDistance(s1, s2,w):\n",
    "    DTW={}\n",
    "    \n",
    "    w = max(w, abs(len(s1)-len(s2)))\n",
    "    \n",
    "    for i in range(-1,len(s1)):\n",
    "        for j in range(-1,len(s2)):\n",
    "            DTW[(i, j)] = float('inf')\n",
    "    DTW[(-1, -1)] = 0\n",
    "  \n",
    "    for i in range(len(s1)):\n",
    "        for j in range(max(0, i-w), min(len(s2), i+w)):\n",
    "            dist= (s1[i]-s2[j])**2\n",
    "            DTW[(i, j)] = dist + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])\n",
    "\t\t\n",
    "    return math.sqrt(DTW[len(s1)-1, len(s2)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math as math\n",
    "\n",
    "def LB_Keogh(s1,s2,r):\n",
    "    LB_sum=0\n",
    "    for ind,i in enumerate(s1):\n",
    "        \n",
    "        lower_bound=min(s2[(ind-r if ind-r>=0 else 0):(ind+r)])\n",
    "        upper_bound=max(s2[(ind-r if ind-r>=0 else 0):(ind+r)])\n",
    "        \n",
    "        if i>upper_bound:\n",
    "            LB_sum=LB_sum+(i-upper_bound)**2\n",
    "        elif i<lower_bound:\n",
    "            LB_sum=LB_sum+(i-lower_bound)**2\n",
    "    \n",
    "    return math.sqrt(LB_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def k_means_clust(data,num_clust,num_iter,w=5):\n",
    "    centroids=random.sample(data,num_clust)\n",
    "    counter=0\n",
    "    for n in range(num_iter):\n",
    "        counter+=1\n",
    "        print counter\n",
    "        assignments={}\n",
    "        #assign data points to clusters\n",
    "        for ind,i in enumerate(data):\n",
    "            min_dist=float('inf')\n",
    "            closest_clust=None\n",
    "            for c_ind,j in enumerate(centroids):\n",
    "                if LB_Keogh(i,j,5)<min_dist:\n",
    "                    cur_dist=DTWDistance(i,j,w)\n",
    "                    if cur_dist<min_dist:\n",
    "                        min_dist=cur_dist\n",
    "                        closest_clust=c_ind\n",
    "            if closest_clust in assignments:\n",
    "                assignments[closest_clust].append(ind)\n",
    "            else:\n",
    "                assignments[closest_clust]=[]\n",
    "                assignments[closest_clust].append(ind)\n",
    "    \n",
    "        #recalculate centroids of clusters\n",
    "        for key in assignments:\n",
    "            clust_sum=0\n",
    "            for k in assignments[key]:\n",
    "                clust_sum=clust_sum+data[k]\n",
    "            centroids[key]=[m/len(assignments[key]) for m in clust_sum]\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    return np.array([(x-df.mean())/(0.5*(df.max()-df.min())) for x in df]).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.array([q.close for q in quotes]).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.array([normalize(x) for x in data]).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10"
     ]
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "centroids=k_means_clust(data,4,10,4)\n",
    "for i in centroids:\n",
    "    \n",
    "    plt.plot(i)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateClasses(arr):\n",
    "    if arr[-1] > arr[-2]:\n",
    "        arr[-1] = 1\n",
    "    else:\n",
    "        arr[-1] = 0\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  62.930885,   64.268064,   65.744534, ...,   65.103801,\n",
       "          64.936653,    0.      ],\n",
       "       [  89.146814,   89.978878,   88.285727, ...,   91.207618,\n",
       "          91.207618,    0.      ],\n",
       "       [ 101.402755,  101.87981 ,  102.700345, ...,  105.152406,\n",
       "         104.837548,    0.      ],\n",
       "       ..., \n",
       "       [ 134.527402,  135.894396,  136.669997, ...,  146.471612,\n",
       "         146.161366,    0.      ],\n",
       "       [  37.732351,   37.645232,   37.432272, ...,   36.289001,\n",
       "          35.978008,    0.      ],\n",
       "       [  38.161422,   38.311886,   38.086188, ...,   37.418505,\n",
       "          36.948305,    0.      ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate Lables\n",
    "data = np.array([generateClasses(q.close) for q in quotes]).astype(np.float)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Shapelet extraction code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_candidates(data, max_len=5, min_len=2):\n",
    "    candidates, l = [], max_len\n",
    "    while l >= min_len:\n",
    "        for i in range(len(data)):\n",
    "            time_serie, label = data[i][0], data[i][1]\n",
    "            for k in range(len(time_serie)-l+1): candidates.append((time_serie[k:k+l], label))\n",
    "        l -= 1\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def check_candidate(data, shapelet):\n",
    "    histogram = {} \n",
    "    for entry in data:\n",
    "        # TODO: entropy pre-pruning in each iteration\n",
    "        time_serie, label = entry[0], entry[1]\n",
    "        d, idx = subsequence_dist(time_serie, shapelet)\n",
    "        if d is not None:\n",
    "            histogram[d] = [(time_serie, label)] if d not in histogram else histogram[d].append((time_serie, label))\n",
    "    return find_best_split_point(histogram)\n",
    "\n",
    "\n",
    "def calculate_dict_entropy(data):\n",
    "    counts = {}\n",
    "    for entry in data:\n",
    "        if entry[1] in counts: counts[entry[1]] += 1\n",
    "        else: counts[entry[1]] = 1\n",
    "    return calculate_entropy(np.divide(list(counts.values()), float(sum(list(counts.values())))))\n",
    "\n",
    "\n",
    "def find_best_split_point(histogram):\n",
    "    histogram_values = list(itertools.chain.from_iterable(list(histogram.values())))\n",
    "    prior_entropy = calculate_dict_entropy(histogram_values)\n",
    "    best_distance, max_ig = 0, 0\n",
    "    best_left, best_right = None, None\n",
    "    for distance in histogram:\n",
    "        data_left = []\n",
    "        data_right = []\n",
    "        for distance2 in histogram:\n",
    "            if distance2 <= distance: data_left.extend(histogram[distance2])\n",
    "            else: data_right.extend(histogram[distance2])\n",
    "        ig = prior_entropy - (float(len(data_left))/float(len(histogram_values))*calculate_dict_entropy(data_left) + \\\n",
    "             float(len(data_right))/float(len(histogram_values)) * calculate_dict_entropy(data_right))\n",
    "        if ig > max_ig: best_distance, max_ig, best_left, best_right = distance, ig, data_left, data_right\n",
    "    return max_ig, best_distance, best_left, best_right\n",
    "\n",
    "\n",
    "def manhattan_distance(a, b, min_dist=float('inf')):\n",
    "    dist = 0\n",
    "    for x, y in zip(a, b):\n",
    "        dist += np.abs(float(x)-float(y))\n",
    "        if dist >= min_dist: return None\n",
    "    return dist\n",
    "\n",
    "def calculate_entropy(probabilities):\n",
    "    return sum([-prob * np.log(prob)/np.log(2) if prob != 0 else 0 for prob in probabilities])\n",
    "\n",
    "\n",
    "def subsequence_dist(time_serie, sub_serie):\n",
    "    if len(sub_serie) < len(time_serie):\n",
    "        min_dist, min_idx = float(\"inf\"), 0\n",
    "        for i in range(len(time_serie)-len(sub_serie)+1):\n",
    "            dist = manhattan_distance(sub_serie, time_serie[i:i+len(sub_serie)], min_dist)\n",
    "            if dist is not None and dist < min_dist: min_dist, min_idx = dist, i\n",
    "        return min_dist, min_idx\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def find_shapelets_bf(data, max_len=100, min_len=1, plot=True, verbose=True):\n",
    "    candidates = generate_candidates(data, max_len, min_len)\n",
    "    bsf_gain, bsf_shapelet = 0, None\n",
    "    if verbose: candidates_length = len(candidates)\n",
    "    for idx, candidate in enumerate(candidates):\n",
    "        gain, dist, data_left, data_right = check_candidate(data, candidate[0])\n",
    "        if verbose: print(idx, '/', candidates_length, \":\", gain, dist)\n",
    "        if gain > bsf_gain:\n",
    "            bsf_gain, bsf_shapelet = gain, candidate[0]\n",
    "            if verbose:\n",
    "                print('Found new best shapelet with gain & dist:', bsf_gain, dist, [x[1] for x in data_left], \\\n",
    "                                                                                   [x[1] for x in data_right])\n",
    "            if plot:\n",
    "                plt.plot(bsf_shapelet)\n",
    "                plt.show()\n",
    "            plt.show()\n",
    "    return bsf_shapelet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
